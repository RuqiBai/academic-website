---
title: Exploring Adversarial Examples via Invertible Neural Networks
subtitle: ''
summary: ''
authors: 
  - Ruqi Bai
  - Saurabh Bagchi
  - David I. Inouye
publication_types: '3'
abstract: 'Adversarial examples (AEs) are images that can mislead deep neural network (DNN) classifiers via introducing slight perturbations into original images.This security vulnerability has led to vast research in recent years because it can introduce real-world threats into systems that rely on neural networks. Yet, a deep understanding of the characteristics of adversarial examples has remained elusive. We propose a new way of achieving such understanding through a recent development, namely, invertible neural models with Lipschitz continuous mapping functions from the input to the output. With the ability to invert any latent representation back to its corresponding input image, we can investigate adversarial examples at a deeper level and disentangle the adversarial example's latent representation. Given this new perspective, we propose a fast latent space adversarial example generation method that could accelerate adversarial training. Moreover, this new perspective could contribute to new ways of adversarial example detection.'

draft: false
featured: false
date: 2021-02-03T17:18:59.197Z

---
